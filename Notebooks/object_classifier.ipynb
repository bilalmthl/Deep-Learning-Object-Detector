{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9fc959",
   "metadata": {},
   "source": [
    "# 🍌🍎 Fruit Classifier — Uses `dataset/` with mapping `negatives -> none`\n",
    "\n",
    "This end-to-end notebook loads images from your structure:\n",
    "```\n",
    "dataset/\n",
    "  apples/  (desk, hand, hard, mixed, plain ... inside)\n",
    "  bananas/ (...)\n",
    "  oranges/ (...)\n",
    "  negatives/ (...)\n",
    "```\n",
    "It maps **`negatives` to the class `none`**, builds `dataset_ready` (train/val/test),\n",
    "trains a ResNet18 classifier, evaluates on test, and provides robust inference\n",
    "on a custom image path (Windows OK).\n",
    "\n",
    "Run top-to-bottom. Edit paths in the Config cell if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bced0b6",
   "metadata": {},
   "source": [
    "## 1) Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c761a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "RAW_ROOT   : C:\\Users\\bilal\\OneDrive\\Projects\\Object Detector\\dataset\n",
      "READY_ROOT : C:\\Users\\bilal\\OneDrive\\Projects\\Object Detector\\dataset_ready\n",
      "ARTIFACTS  : C:\\Users\\bilal\\OneDrive\\Projects\\Object Detector\\artifacts\n",
      "Target classes: ['apples', 'bananas', 'none', 'oranges']\n"
     ]
    }
   ],
   "source": [
    "# --- Imports\n",
    "import os, stat, shutil, time, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms as T\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Be resilient to slightly truncated jpgs\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# --- Config (EDIT ME) ---\n",
    "RAW_ROOT   = Path('dataset')          # << Your folder shown in VS Code\n",
    "READY_ROOT = Path('dataset_ready')    # will be created\n",
    "ARTIFACTS  = Path('artifacts'); ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Map your source folders to the *target* class names used for training\n",
    "SRC_TO_DST = {\n",
    "    'apples': 'apples',\n",
    "    'bananas': 'bananas',\n",
    "    'oranges': 'oranges',\n",
    "    'negatives': 'none',   # <- map negatives to the class 'none'\n",
    "}\n",
    "\n",
    "# TARGET class list (order doesn't matter here; ImageFolder will set final order)\n",
    "TARGET_CLASSES = sorted(set(SRC_TO_DST.values()))\n",
    "\n",
    "# Training hyperparams\n",
    "BASE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "USE_PRETRAINED = True\n",
    "FREEZE_BACKBONE_EPOCHS = 0\n",
    "\n",
    "print('RAW_ROOT   :', RAW_ROOT.resolve())\n",
    "print('READY_ROOT :', READY_ROOT.resolve())\n",
    "print('ARTIFACTS  :', ARTIFACTS.resolve())\n",
    "print('Target classes:', TARGET_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34e602",
   "metadata": {},
   "source": [
    "## 2) Windows-safe delete and optional dataset health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2441d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_rmtree(path, retries=10, delay=0.3):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return\n",
    "    def onerror(func, p, exc_info):\n",
    "        try:\n",
    "            os.chmod(p, stat.S_IWRITE)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            func(p)\n",
    "        except PermissionError:\n",
    "            raise\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            shutil.rmtree(path, onerror=onerror)\n",
    "            return\n",
    "        except PermissionError:\n",
    "            time.sleep(delay)\n",
    "    trash = path.with_name(path.name + f'._trash_{int(time.time())}')\n",
    "    path.rename(trash)\n",
    "    print(f'[safe_rmtree] Could not remove {path}. Renamed to {trash}. Delete it later.')\n",
    "\n",
    "def dataset_health_check(root: Path, quarantine: Path):\n",
    "    root, quarantine = Path(root), Path(quarantine)\n",
    "    quarantine.mkdir(parents=True, exist_ok=True)\n",
    "    total = bad = 0\n",
    "    for p in root.rglob('*'):\n",
    "        if p.suffix.lower() not in ('.jpg','.jpeg','.png','.bmp'):\n",
    "            continue\n",
    "        total += 1\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                im.verify()\n",
    "            with Image.open(p) as im:\n",
    "                _ = im.tobytes()\n",
    "        except Exception as e:\n",
    "            bad += 1\n",
    "            dest = quarantine / p.name\n",
    "            try:\n",
    "                shutil.move(str(p), str(dest))\n",
    "                print(f'[health_check] Moved bad image -> {dest}')\n",
    "            except Exception as ee:\n",
    "                print(f'[health_check] Could not move {p}: {ee}')\n",
    "    print(f'[health_check] Scanned {total}, quarantined {bad}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ef959",
   "metadata": {},
   "source": [
    "## 3) Build `dataset_ready` with mapping (negatives -> none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8e1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_ready_map(\n",
    "    src_root: Path,\n",
    "    dst_root: Path,\n",
    "    src_to_dst: Dict[str, str],\n",
    "    val_ratio: float = 0.15,\n",
    "    test_ratio: float = 0.15,\n",
    "    resize_to: int = 224,\n",
    "    exts: Tuple[str, ...] = ('.jpg','.jpeg','.png','.bmp'),\n",
    "):\n",
    "    src_root, dst_root = Path(src_root), Path(dst_root)\n",
    "    # Remove previous ready folder\n",
    "    if dst_root.exists():\n",
    "        print(f'[dataset_ready] Removing existing {dst_root}')\n",
    "        safe_rmtree(dst_root)\n",
    "    # Create destination class folders for each split\n",
    "    dst_classes = sorted(set(src_to_dst.values()))\n",
    "    for split in ['train','val','test']:\n",
    "        for cls in dst_classes:\n",
    "            (dst_root / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each SOURCE class folder and place files into DEST class\n",
    "    for src_cls, dst_cls in src_to_dst.items():\n",
    "        src_dir = src_root / src_cls\n",
    "        assert src_dir.exists(), f'Missing source folder: {src_dir}'\n",
    "        files = sorted([p for p in src_dir.rglob('*') if p.suffix.lower() in exts])\n",
    "        random.shuffle(files)\n",
    "        n = len(files)\n",
    "        n_test = int(n * test_ratio)\n",
    "        n_val  = int(n * val_ratio)\n",
    "        n_train = n - n_val - n_test\n",
    "        splits = {\n",
    "            'train': files[:n_train],\n",
    "            'val'  : files[n_train:n_train+n_val],\n",
    "            'test' : files[n_train+n_val:],\n",
    "        }\n",
    "        print(f'{src_cls} -> {dst_cls}: train={len(splits[\"train\"])}, val={len(splits[\"val\"])}, test={len(splits[\"test\"])}')\n",
    "        kept = skipped = 0\n",
    "        for split, paths in splits.items():\n",
    "            for i, src in enumerate(paths, 1):\n",
    "                try:\n",
    "                    with Image.open(src) as im:\n",
    "                        im = ImageOps.exif_transpose(im).convert('RGB')\n",
    "                        if resize_to:\n",
    "                            im = im.resize((resize_to, resize_to), Image.BILINEAR)\n",
    "                        dst = dst_root / split / dst_cls / f'{src.stem}_{i:05d}.jpg'\n",
    "                        im.save(dst, quality=95, optimize=True)\n",
    "                        with Image.open(dst) as chk:\n",
    "                            chk.verify()\n",
    "                    kept += 1\n",
    "                except Exception:\n",
    "                    skipped += 1\n",
    "        print(f'{src_cls} -> {dst_cls}: kept={kept}, skipped={skipped}')\n",
    "    print('[dataset_ready] Done ->', dst_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff3c34",
   "metadata": {},
   "source": [
    "## 4) Prepare dataset (optional health check -> build `dataset_ready`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b47c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[health_check] Scanned 2548, quarantined 0.\n",
      "[dataset_ready] Removing existing dataset_ready\n",
      "apples -> apples: train=430, val=91, test=91\n",
      "apples -> apples: kept=612, skipped=0\n",
      "bananas -> bananas: train=546, val=117, test=117\n",
      "bananas -> bananas: kept=780, skipped=0\n",
      "oranges -> oranges: train=449, val=96, test=96\n",
      "oranges -> oranges: kept=641, skipped=0\n",
      "negatives -> none: train=361, val=77, test=77\n",
      "negatives -> none: kept=515, skipped=0\n",
      "[dataset_ready] Done -> dataset_ready\n"
     ]
    }
   ],
   "source": [
    "# Optional health check\n",
    "QUAR = RAW_ROOT.parent / 'quarantine_bad_images'\n",
    "if RAW_ROOT.exists():\n",
    "    dataset_health_check(RAW_ROOT, QUAR)\n",
    "else:\n",
    "    print('[warn] RAW_ROOT not found:', RAW_ROOT)\n",
    "\n",
    "# Build ready dataset using mapping (negatives -> none)\n",
    "dataset_ready_map(RAW_ROOT, READY_ROOT, SRC_TO_DST, resize_to=224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f31769",
   "metadata": {},
   "source": [
    "## 5) Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2597c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageFolder class order: ['apples', 'bananas', 'none', 'oranges']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1786, 381, 381)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(size=BASE_SIZE, scale=(0.7, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(BASE_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(READY_ROOT/'train', transform=train_transform)\n",
    "val_ds   = datasets.ImageFolder(READY_ROOT/'val',   transform=eval_transform)\n",
    "test_ds  = datasets.ImageFolder(READY_ROOT/'test',  transform=eval_transform)\n",
    "\n",
    "# Trust ImageFolder's class order\n",
    "CLASS_ORDER = train_ds.classes\n",
    "print('ImageFolder class order:', CLASS_ORDER)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c4661",
   "metadata": {},
   "source": [
    "## 6) Build model (ResNet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a19296c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(CLASS_ORDER)\n",
    "if USE_PRETRAINED:\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "else:\n",
    "    model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "print(model.fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49be7ad",
   "metadata": {},
   "source": [
    "## 7) Train with validation and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0160fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Train loss 0.3412 acc 0.8796 | Val loss 0.2036 acc 0.9291\n",
      "  -> saved new best to artifacts\\best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/10 | Train loss 0.1709 acc 0.9451 | Val loss 0.4023 acc 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/10 | Train loss 0.1589 acc 0.9518 | Val loss 0.1452 acc 0.9580\n",
      "  -> saved new best to artifacts\\best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/10 | Train loss 0.1027 acc 0.9681 | Val loss 0.1205 acc 0.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/10 | Train loss 0.0826 acc 0.9714 | Val loss 0.1228 acc 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/10 | Train loss 0.0576 acc 0.9804 | Val loss 0.0837 acc 0.9738\n",
      "  -> saved new best to artifacts\\best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/10 | Train loss 0.0516 acc 0.9849 | Val loss 0.0492 acc 0.9816\n",
      "  -> saved new best to artifacts\\best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/10 | Train loss 0.0419 acc 0.9854 | Val loss 0.0596 acc 0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/10 | Train loss 0.0246 acc 0.9922 | Val loss 0.0489 acc 0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train loss 0.0231 acc 0.9899 | Val loss 0.0482 acc 0.9843\n",
      "  -> saved new best to artifacts\\best_model.pth\n",
      "Best val acc: 0.984251968503937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(EPOCHS, 1))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for images, labels in tqdm(loader, leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels).item()\n",
    "        total_loss += loss * images.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "    return total_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for images, labels in tqdm(train_loader, leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "    return total_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "best_val = 0.0\n",
    "BEST_PATH = ARTIFACTS / 'best_model.pth'\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if epoch == 1 and FREEZE_BACKBONE_EPOCHS > 0:\n",
    "        for name, p in model.named_parameters():\n",
    "            if not name.startswith('fc.'):\n",
    "                p.requires_grad = False\n",
    "    if epoch == FREEZE_BACKBONE_EPOCHS + 1:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    tr_loss, tr_acc = train_one_epoch()\n",
    "    va_loss, va_acc = evaluate(val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch:>2}/{EPOCHS} | Train loss {tr_loss:.4f} acc {tr_acc:.4f} | Val loss {va_loss:.4f} acc {va_acc:.4f}')\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print('  -> saved new best to', BEST_PATH)\n",
    "print('Best val acc:', best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fa6ca",
   "metadata": {},
   "source": [
    "## 8) Test evaluation (load best checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8acb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.0406 | Test acc 0.9816\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[ 89   0   2   0]\n",
      " [  0 116   1   0]\n",
      " [  0   2  75   0]\n",
      " [  0   2   0  94]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "BEST_PATH = ARTIFACTS / 'best_model.pth'\n",
    "if BEST_PATH.exists():\n",
    "    model.load_state_dict(torch.load(BEST_PATH, map_location=device))\n",
    "    model.to(device).eval()\n",
    "else:\n",
    "    print('[warn] Best model not found, evaluating current weights.')\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_with_confusion(loader, n_classes):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for images, labels in tqdm(loader, leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels).item()\n",
    "        total_loss += loss * images.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        for t, p in zip(labels.view(-1).cpu().numpy(), preds.view(-1).cpu().numpy()):\n",
    "            cm[t, p] += 1\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "    return total_loss / max(total,1), correct / max(total,1), cm\n",
    "\n",
    "te_loss, te_acc, cm = evaluate_with_confusion(test_loader, len(CLASS_ORDER))\n",
    "print(f'Test loss {te_loss:.4f} | Test acc {te_acc:.4f}')\n",
    "print('Confusion matrix (rows=true, cols=pred):\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa44071",
   "metadata": {},
   "source": [
    "## 9) Inference on a custom image (Windows path OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665521ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE_PATH: C:\\Users\\bilal\\OneDrive\\Projects\\Object Detector\\Sample\\test_image.jpg\n",
      "Strict center-crop -> none\n",
      "Probs: {'apples': 0.013024981133639812, 'bananas': 6.282403046498075e-05, 'none': 0.986713707447052, 'oranges': 0.00019841204630210996}\n",
      "Multi-scale center -> none | crop box: (80, 0, 560, 480)\n",
      "Probs: {'apples': 0.01567905955016613, 'bananas': 7.926952093839645e-05, 'none': 0.9840013980865479, 'oranges': 0.00024020539422053844}\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def _predict_tensor(x):\n",
    "    model.eval()\n",
    "    logits = model(x.to(device))\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "    return int(probs.argmax()), probs\n",
    "\n",
    "# Must match training eval transform\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(BASE_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def predict_image_fixed(path: str):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    x = eval_transform(img).unsqueeze(0)\n",
    "    idx, probs = _predict_tensor(x)\n",
    "    return CLASS_ORDER[idx], probs\n",
    "\n",
    "def predict_image_multiscale_center(path: str, scales=(1.0, 0.85, 0.7)):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    best = None\n",
    "    for s in scales:\n",
    "        side = int(min(w, h) * s)\n",
    "        left, top = max(0, (w - side)//2), max(0, (h - side)//2)\n",
    "        crop = img.crop((left, top, left + side, top + side))\n",
    "        x = eval_transform(crop).unsqueeze(0)\n",
    "        idx, probs = _predict_tensor(x)\n",
    "        conf = float(probs[idx])\n",
    "        if best is None or conf > best['conf']:\n",
    "            best = {'idx': idx, 'probs': probs, 'conf': conf, 'box': (left, top, left + side, top + side)}\n",
    "    return CLASS_ORDER[best['idx']], best['probs'], best['box']\n",
    "\n",
    "# Set your test image path here (raw string for Windows backslashes)\n",
    "EXAMPLE_PATH = r\"C:\\Users\\bilal\\OneDrive\\Projects\\Object Detector\\samples\\test_image.jpg\"\n",
    "print('EXAMPLE_PATH:', EXAMPLE_PATH)\n",
    "\n",
    "labelA, probsA = predict_image_fixed(EXAMPLE_PATH)\n",
    "print('Strict center-crop ->', labelA)\n",
    "print('Probs:', {cls: float(probsA[i]) for i, cls in enumerate(CLASS_ORDER)})\n",
    "\n",
    "labelB, probsB, box = predict_image_multiscale_center(EXAMPLE_PATH)\n",
    "print('Multi-scale center ->', labelB, '| crop box:', box)\n",
    "print('Probs:', {cls: float(probsB[i]) for i, cls in enumerate(CLASS_ORDER)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33f3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook class order: ['apples', 'bananas', 'none', 'oranges']\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook class order:\", CLASS_ORDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd62926",
   "metadata": {},
   "source": [
    "## 10) Notes\n",
    "- The mapping lets you keep your current folder names; `negatives` becomes the training class `none`.\n",
    "- You can add more nested subfolders under each class; they are all included.\n",
    "- If Windows locks `dataset_ready`, the safe remover will retry or rename and continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
